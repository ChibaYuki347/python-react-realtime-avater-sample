# ====================================================================================================
# RAGã‚µãƒ¼ãƒ“ã‚¹ - æ¤œç´¢æ‹¡å¼µç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
# ====================================================================================================

import os
import json
import asyncio
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
from datetime import datetime
from pathlib import Path

# OpenAI SDK
from openai import AsyncAzureOpenAI

# è‡ªä½œã‚µãƒ¼ãƒ“ã‚¹
from .document_service import DocumentService, Document, DocumentChunk

@dataclass
class RAGQuery:
    """RAGã‚¯ã‚¨ãƒªãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    user_id: str
    query: str
    conversation_id: Optional[str] = None
    max_results: int = 5
    include_metadata: bool = True

@dataclass
class RAGResponse:
    """RAGå¿œç­”ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    query: str
    answer: str
    relevant_chunks: List[DocumentChunk]
    conversation_id: str
    timestamp: str
    metadata: Dict[str, any] = None

class SimpleEmbeddingService:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆå¾Œã§Azure AI Searchã«ç½®ãæ›ãˆäºˆå®šï¼‰"""
    
    def __init__(self):
        self.documents: List[Document] = []
        self.chunks: List[DocumentChunk] = []
        self._initialized = False
    
    async def initialize(self):
        """ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–"""
        if self._initialized:
            return
        
        doc_service = DocumentService()
        self.documents, self.chunks = await doc_service.get_all_documents_with_chunks()
        self._initialized = True
        logging.info(f"RAGæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†: {len(self.documents)} documents, {len(self.chunks)} chunks")
    
    async def search_chunks(self, query: str, max_results: int = 5) -> List[DocumentChunk]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“æ¤œç´¢"""
        await self.initialize()
        
        query_lower = query.lower()
        results = []
        
        for chunk in self.chunks:
            # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            content_lower = chunk.content.lower()
            
            # ã‚¯ã‚¨ãƒªã«å«ã¾ã‚Œã‚‹å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹
            score = 0
            for word in query_lower.split():
                score += content_lower.count(word)
            
            if score > 0:
                results.append((chunk, score))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[1], reverse=True)
        
        # ä¸Šä½çµæœã‚’è¿”ã™
        return [chunk for chunk, score in results[:max_results]]

class RAGService:
    """RAG (Retrieval Augmented Generation) ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        # Azure OpenAIè¨­å®š
        self.azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        self.azure_openai_key = os.getenv("AZURE_OPENAI_KEY") or os.getenv("AZURE_OPENAI_API_KEY")
        self.azure_openai_deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4-1")
        self.use_managed_identity = os.getenv("USE_MANAGED_IDENTITY", "false").lower() == "true"
        
        self.logger = logging.getLogger(__name__)
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.openai_client = None
        if self.azure_openai_endpoint:
            try:
                if self.azure_openai_key and not self.use_managed_identity:
                    # API Keyèªè¨¼
                    self.openai_client = AsyncAzureOpenAI(
                        azure_endpoint=self.azure_openai_endpoint,
                        api_key=self.azure_openai_key,
                        api_version="2024-05-01-preview"
                    )
                    self.logger.info(f"RAG: Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ (API Key): {self.azure_openai_endpoint}")
                else:
                    # Managed Identityèªè¨¼
                    from azure.identity.aio import DefaultAzureCredential as AsyncDefaultAzureCredential
                    
                    credential = AsyncDefaultAzureCredential()
                    
                    async def get_token():
                        token = await credential.get_token("https://cognitiveservices.azure.com/.default")
                        return token.token
                    
                    self.openai_client = AsyncAzureOpenAI(
                        azure_ad_token_provider=get_token,
                        api_version="2024-05-01-preview",
                        azure_endpoint=self.azure_openai_endpoint
                    )
                    self.logger.info(f"RAG: Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ (Managed Identity): {self.azure_openai_endpoint}")
            except Exception as e:
                self.logger.error(f"RAG: Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        else:
            self.logger.warning("RAG: AZURE_OPENAI_ENDPOINT ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹
        self.search_service = SimpleEmbeddingService()
    
    async def generate_rag_response(self, rag_query: RAGQuery) -> RAGResponse:
        """RAGå¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # 1. é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
            relevant_chunks = await self.search_service.search_chunks(
                rag_query.query, 
                rag_query.max_results
            )
            
            # 2. GPT-4.1ã‚’ä½¿ã£ã¦å¿œç­”ã‚’ç”Ÿæˆ
            answer = await self._generate_answer_with_context(
                rag_query.query, 
                relevant_chunks
            )
            
            # 3. RAGå¿œç­”ã‚’æ§‹ç¯‰
            response = RAGResponse(
                query=rag_query.query,
                answer=answer,
                relevant_chunks=relevant_chunks,
                conversation_id=rag_query.conversation_id or self._generate_conversation_id(),
                timestamp=datetime.now().isoformat(),
                metadata={
                    "chunks_used": len(relevant_chunks),
                    "model": self.azure_openai_deployment,
                    "user_id": rag_query.user_id
                }
            )
            
            self.logger.info(f"RAGå¿œç­”ç”ŸæˆæˆåŠŸ: {rag_query.user_id}")
            return response
            
        except Exception as e:
            self.logger.error(f"RAGå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”
            return RAGResponse(
                query=rag_query.query,
                answer="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚",
                relevant_chunks=[],
                conversation_id=rag_query.conversation_id or self._generate_conversation_id(),
                timestamp=datetime.now().isoformat(),
                metadata={"error": str(e)}
            )
    
    async def _generate_answer_with_context(self, query: str, chunks: List[DocumentChunk]) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ã£ã¦GPT-4.1ã§å¿œç­”ç”Ÿæˆ"""
        if not self.openai_client:
            # é–‹ç™ºç’°å¢ƒç”¨ã®ãƒ¢ãƒƒã‚¯å¿œç­”
            if not chunks:
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Azure OpenAIæ¥ç¶šãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ¢ãƒƒã‚¯å¿œç­”ã‚’è¿”ã—ã¦ã„ã¾ã™ã€‚"
            
            # ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã«åŸºã¥ã„ãŸç°¡æ˜“å¿œç­”
            context_summary = "\n".join([f"â€¢ {chunk.content[:150]}..." for chunk in chunks[:3]])
            return f"""
ã€é–‹ç™ºç’°å¢ƒã§ã® RAGæ¤œç´¢çµæœã€‘

ã€Œ{query}ã€ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ¤œç´¢ã—ã¾ã—ãŸï¼š

{context_summary}

ğŸ“‹ **æ¤œç´¢çµæœè©³ç´°**:
- è¦‹ã¤ã‹ã£ãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}
- ä¸»è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {', '.join(set([chunk.metadata.get('document_title', 'ä¸æ˜')[:20] for chunk in chunks[:3]]))}
- é–¢é€£ã‚«ãƒ†ã‚´ãƒª: {', '.join(set([chunk.metadata.get('document_category', '') for chunk in chunks[:3] if chunk.metadata.get('document_category')]))}

ğŸ’¡ **å®Œå…¨ãªAIå¿œç­”ã‚’å¾—ã‚‹ã«ã¯**: Azure OpenAI ã‚µãƒ¼ãƒ“ã‚¹æ¥ç¶šãŒå¿…è¦ã§ã™ã€‚ç¾åœ¨ã¯æ¤œç´¢æ©Ÿèƒ½ã®ã¿å‹•ä½œä¸­ã§ã™ã€‚

ã‚ˆã‚Šè©³ã—ã„æƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚
            """.strip()
        
        if not chunks:
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = "\n\n".join([
            f"ã€{chunk.metadata.get('document_title', 'ä¸æ˜')}ã€‘\n{chunk.content}"
            for chunk in chunks
        ])
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        system_prompt = """
ã‚ãªãŸã¯Pythonã¨Reactã‚’ä½¿ã£ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒã‚¿ãƒ¼ã‚¢ãƒ—ãƒªã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æŠ€è¡“ä»•æ§˜ã¨å®Ÿè£…ã‚¬ã‚¤ãƒ‰ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§å…·ä½“çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªæŒ‡é‡:
1. æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«å›ç­”ã™ã‚‹
2. Azure Speech Serviceã€FastAPIã€Reactã®æŠ€è¡“çš„è©³ç´°ã«ç²¾é€šã—ã¦ã„ã‚‹
3. å®Ÿè£…ã«å½¹ç«‹ã¤å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ã‚„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã™ã‚‹
4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®ã“ã¨ã‚’æ˜ç¢ºã«ä¼ãˆã‚‹
5. å›ç­”ã¯æ—¥æœ¬èªã§ã€æŠ€è¡“çš„ã«æ­£ç¢ºã§ç†è§£ã—ã‚„ã™ãèª¬æ˜ã™ã‚‹
"""
        
        user_prompt = f"""
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š

ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{query}

ã€å›ç­”ã®æŒ‡é‡ã€‘
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æ´»ç”¨ã—ã¦å…·ä½“çš„ã«å›ç­”ã™ã‚‹
- æŠ€è¡“çš„ãªå†…å®¹ã¯å®Ÿè£…ä¾‹ã‚„ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å«ã‚ã‚‹
- é–¢é€£ã™ã‚‹ Azure ã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®šã‚„ä½¿ã„æ–¹ãŒã‚ã‚Œã°èª¬æ˜ã™ã‚‹
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€ã©ã®ã‚ˆã†ãªæƒ…å ±ãŒå¿…è¦ã‹ã‚’ææ¡ˆã™ã‚‹
"""
        
        try:
            # Azure OpenAI APIã‚’å‘¼ã³å‡ºã—
            response = await self.openai_client.chat.completions.create(
                model=self.azure_openai_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            self.logger.error(f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def _generate_conversation_id(self) -> str:
        """ä¼šè©±IDã‚’ç”Ÿæˆ"""
        import uuid
        return str(uuid.uuid4())
    
    async def get_available_documents(self) -> List[Document]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’å–å¾—"""
        await self.search_service.initialize()
        return self.search_service.documents
    
    async def search_documents_only(self, query: str, max_results: int = 10) -> List[DocumentChunk]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã®ã¿ï¼ˆGPTå¿œç­”ãªã—ï¼‰"""
        return await self.search_service.search_chunks(query, max_results)

# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_rag_service():
    """RAGã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    rag_service = RAGService()
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æŠ€è¡“æ§‹æˆã«ã¤ã„ã¦æ•™ãˆã¦",
        "Azure Speech Serviceã®è¨­å®šæ–¹æ³•ã¯ï¼Ÿ",
        "Reactã§ã‚¢ãƒã‚¿ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹æ–¹æ³•",
        "FastAPIã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ã¤ã„ã¦"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"è³ªå•: {query}")
        print(f"{'='*50}")
        
        rag_query = RAGQuery(
            user_id="test_user",
            query=query,
            max_results=3
        )
        
        response = await rag_service.generate_rag_response(rag_query)
        print(f"å›ç­”: {response.answer}")
        print(f"ä½¿ç”¨ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(response.relevant_chunks)}")

if __name__ == "__main__":
    asyncio.run(test_rag_service())